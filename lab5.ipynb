{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# https://www.python.org/dev/peps/pep-0008#introduction<BR>\n",
    "# http://scikit-learn.org/<BR>\n",
    "# http://pandas.pydata.org/<BR>\n",
    "\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetch the data and load it in pandas\n",
    "data = pd.read_csv('train.csv')\n",
    "print(\"Size of the data: \", data.shape)\n",
    "\n",
    "#%%\n",
    "# See data (five rows) using pandas tools\n",
    "#print data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare input to scikit and train and test cut\n",
    "\n",
    "binary_data = data[np.logical_or(data['Cover_Type'] == 1, data['Cover_Type'] == 2)] # two-class classification set\n",
    "X = binary_data.drop('Cover_Type', axis=1).values\n",
    "y = binary_data['Cover_Type'].values\n",
    "print(np.unique(y))\n",
    "y = 2 * y - 3 # converting labels from [1,2] to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import cross validation tools from scikit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "### Train a single decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(max_depth=8)\n",
    "\n",
    "# Train the classifier and print training time\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Do classification on the test dataset and print classification results\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = data['Cover_Type'].unique().astype(str).sort()\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Compute accuracy of the classifier (correctly classified instances)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================================\n",
    "#%%\n",
    "### Train AdaBoost\n",
    "\n",
    "# Your first exercise is to program AdaBoost.\n",
    "# You can call 'DecisionTreeClassifier' as above, \n",
    "# but you have to figure out how to pass the weight vector (for weighted classification) \n",
    "# to the <code>fit</code> function using the help pages of scikit-learn. At the end of \n",
    "# the loop, compute the training and test errors so the last section of the code can \n",
    "# plot the learning curves. \n",
    "# \n",
    "# Once the code is finished, play around with the hyperparameters (D and T), \n",
    "# and try to understand what is happening.\n",
    "\n",
    "D = 2 # tree depth\n",
    "T = 10 # number of trees\n",
    "w = np.ones(X_train.shape[0]) / X_train.shape[0] # weight initialization\n",
    "training_scores = np.zeros(X_train.shape[0]) # init scores with 0\n",
    "test_scores     = np.zeros(X_test.shape[0])\n",
    "\n",
    "# init errors\n",
    "training_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# we will make T trees\n",
    "for t in range(T):\n",
    "    # initialize the classifier (i.e. the tree)\n",
    "    clf = DecisionTreeClassifier(max_depth=D)\n",
    "    # fit with weights\n",
    "    clf.fit(X_train, y_train, sample_weight=w)\n",
    "    # predict with tree\n",
    "    y_pred = clf.predict(X_train)\n",
    "    # check where clf was wrong\n",
    "    indicator = np.not_equal(y_pred, y_train)\n",
    "    # calculate gamma and alpha with equations from doc\n",
    "    gamma = w[indicator].sum() / w.sum()\n",
    "    alpha = np.log((1-gamma) / gamma)\n",
    "    # update weights\n",
    "    w *= np.exp(alpha * indicator) \n",
    "    \n",
    "    # calculate scores and errors for this tree\n",
    "    training_scores += alpha * y_pred\n",
    "    training_error = 1. * len(training_scores[training_scores * y_train < 0]) / len(X_train)\n",
    "    # calculate test error and score\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    test_scores += alpha * y_test_pred\n",
    "    test_error = 1. * len(test_scores[test_scores * y_test < 0]) / len(X_test)\n",
    "    #print(t, \": \", alpha, gamma, training_error, test_error)\n",
    "    \n",
    "    training_errors.append(training_error)\n",
    "    test_errors.append(test_error)\n",
    "    \n",
    "plt.plot(training_errors, label=\"training error\")\n",
    "plt.plot(test_errors, label=\"test error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================================\n",
    "#%%\n",
    "### Optimize AdaBoost\n",
    "\n",
    "# Your final exercise is to optimize the tree depth in AdaBoost. \n",
    "# Copy-paste your AdaBoost code into a function, and call it with different tree depths \n",
    "# and, for simplicity, with T = 100 iterations (number of trees). Plot the final \n",
    "# test error vs the tree depth. Discuss the plot.\n",
    "\n",
    "def AdaBoost(D, T):\n",
    "    w = np.ones(X_train.shape[0]) / X_train.shape[0]\n",
    "    training_scores = np.zeros(X_train.shape[0])\n",
    "    test_scores = np.zeros(X_test.shape[0])\n",
    "\n",
    "    ts = plt.arange(len(training_scores))\n",
    "    training_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for t in range(T):\n",
    "        clf = DecisionTreeClassifier(max_depth=D)\n",
    "        clf.fit(X_train, y_train, sample_weight=w)\n",
    "        y_pred = clf.predict(X_train)\n",
    "        \n",
    "        indicator = np.not_equal(y_pred, y_train)\n",
    "        gamma = w[indicator].sum() / w.sum()\n",
    "        alpha = np.log((1-gamma) / gamma)\n",
    "        w *= np.exp(alpha * indicator) \n",
    "\n",
    "        training_scores += alpha * y_pred\n",
    "        training_error = 1. * len(training_scores[training_scores * y_train < 0]) / len(X_train)\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        test_scores += alpha * y_test_pred\n",
    "        test_error = 1. * len(test_scores[test_scores * y_test < 0]) / len(X_test)\n",
    "        #print(t, \": \", alpha, gamma, training_error, test_error)\n",
    "\n",
    "        plt.clf()\n",
    "        training_errors.append(training_error)\n",
    "        test_errors.append(test_error)\n",
    "        #plt.show()\n",
    "        #plt.title(\"Depth = \" + str(D))\n",
    "        #plt.plot(ts[:t+1], training_errors[:t+1])\n",
    "        #plt.plot(ts[:t+1], test_errors[:t+1])\n",
    "        #display.clear_output(wait=True)\n",
    "        #display.display(plt.gcf())\n",
    "        #sleep(.001)\n",
    "    return test_error\n",
    "\n",
    "\n",
    "\n",
    "Ds = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "final_test_errors = []\n",
    "for D in Ds:\n",
    "    print(D)\n",
    "    final_test_errors.append(AdaBoost(D, 100))\n",
    "\n",
    "\n",
    "plt.plot(Ds, final_test_errors)\n",
    "plt.title(\"Test error vs. tree depth\")\n",
    "plt.ylabel(\"Test error\")\n",
    "plt.xlabel(\"Tree depth\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
